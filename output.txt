model(
   (Linear0): Linear(row=2, column=8, isLastLayer=False)
   (Linear1): Linear(row=8, column=1, isLastLayer=True)
)
Epoch 0: training-loss: 0.696 | training-acc: 0.490 | test-loss: 0.698 | test-acc: 0.500
Epoch 1000: training-loss: 0.541 | training-acc: 0.750 | test-loss: 0.571 | test-acc: 0.690
Epoch 2000: training-loss: 0.363 | training-acc: 0.900 | test-loss: 0.449 | test-acc: 0.790
Epoch 3000: training-loss: 0.266 | training-acc: 0.910 | test-loss: 0.409 | test-acc: 0.810
Epoch 4000: training-loss: 0.229 | training-acc: 0.900 | test-loss: 0.407 | test-acc: 0.800
Epoch 5000: training-loss: 0.216 | training-acc: 0.900 | test-loss: 0.419 | test-acc: 0.810
Epoch 6000: training-loss: 0.209 | training-acc: 0.900 | test-loss: 0.430 | test-acc: 0.800
Epoch 7000: training-loss: 0.206 | training-acc: 0.890 | test-loss: 0.438 | test-acc: 0.800
Epoch 8000: training-loss: 0.204 | training-acc: 0.890 | test-loss: 0.445 | test-acc: 0.800
Epoch 9000: training-loss: 0.202 | training-acc: 0.900 | test-loss: 0.450 | test-acc: 0.800
Epoch 10000: training-loss: 0.202 | training-acc: 0.900 | test-loss: 0.454 | test-acc: 0.800
model(
   (Linear0): Linear(row=2, column=16, isLastLayer=False)
   (Linear1): Linear(row=16, column=1, isLastLayer=True)
)
Epoch 0: training-loss: 0.724 | training-acc: 0.480 | test-loss: 0.725 | test-acc: 0.510
Epoch 1000: training-loss: 0.382 | training-acc: 0.890 | test-loss: 0.456 | test-acc: 0.820
Epoch 2000: training-loss: 0.249 | training-acc: 0.900 | test-loss: 0.394 | test-acc: 0.830
Epoch 3000: training-loss: 0.207 | training-acc: 0.900 | test-loss: 0.398 | test-acc: 0.830
Epoch 4000: training-loss: 0.187 | training-acc: 0.900 | test-loss: 0.410 | test-acc: 0.820
Epoch 5000: training-loss: 0.175 | training-acc: 0.910 | test-loss: 0.423 | test-acc: 0.820
Epoch 6000: training-loss: 0.168 | training-acc: 0.910 | test-loss: 0.442 | test-acc: 0.810
Epoch 7000: training-loss: 0.163 | training-acc: 0.910 | test-loss: 0.456 | test-acc: 0.810
Epoch 8000: training-loss: 0.159 | training-acc: 0.910 | test-loss: 0.466 | test-acc: 0.820
Epoch 9000: training-loss: 0.157 | training-acc: 0.920 | test-loss: 0.475 | test-acc: 0.820
Epoch 10000: training-loss: 0.155 | training-acc: 0.920 | test-loss: 0.483 | test-acc: 0.820
model(
   (Linear0): Linear(row=2, column=8, isLastLayer=False)
   (Linear1): Linear(row=8, column=8, isLastLayer=False)
   (Linear2): Linear(row=8, column=1, isLastLayer=True)
)
Epoch 0: training-loss: 0.700 | training-acc: 0.540 | test-loss: 0.699 | test-acc: 0.510
Epoch 1000: training-loss: 0.239 | training-acc: 0.920 | test-loss: 0.424 | test-acc: 0.750
Epoch 2000: training-loss: 0.210 | training-acc: 0.910 | test-loss: 0.513 | test-acc: 0.780
Epoch 3000: training-loss: 0.181 | training-acc: 0.920 | test-loss: 0.504 | test-acc: 0.780
Epoch 4000: training-loss: 0.165 | training-acc: 0.920 | test-loss: 0.512 | test-acc: 0.820
Epoch 5000: training-loss: 0.157 | training-acc: 0.920 | test-loss: 0.523 | test-acc: 0.820
Epoch 6000: training-loss: 0.151 | training-acc: 0.930 | test-loss: 0.533 | test-acc: 0.820
Epoch 7000: training-loss: 0.151 | training-acc: 0.910 | test-loss: 0.549 | test-acc: 0.840
Epoch 8000: training-loss: 0.146 | training-acc: 0.920 | test-loss: 0.563 | test-acc: 0.850
Epoch 9000: training-loss: 0.142 | training-acc: 0.920 | test-loss: 0.578 | test-acc: 0.850
Epoch 10000: training-loss: 0.137 | training-acc: 0.920 | test-loss: 0.593 | test-acc: 0.850
Epoch 0: training-loss: 0.694 | training-acc: 0.480 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.572 | training-acc: 0.860 | test-loss: 0.584 | test-acc: 0.780
Epoch 2000: training-loss: 0.389 | training-acc: 0.840 | test-loss: 0.453 | test-acc: 0.780
Epoch 3000: training-loss: 0.304 | training-acc: 0.840 | test-loss: 0.422 | test-acc: 0.780
Epoch 4000: training-loss: 0.215 | training-acc: 0.960 | test-loss: 0.413 | test-acc: 0.780
Epoch 5000: training-loss: 0.179 | training-acc: 0.960 | test-loss: 0.454 | test-acc: 0.800
Epoch 6000: training-loss: 0.158 | training-acc: 0.960 | test-loss: 0.497 | test-acc: 0.800
Epoch 7000: training-loss: 0.144 | training-acc: 0.960 | test-loss: 0.541 | test-acc: 0.800
Epoch 8000: training-loss: 0.136 | training-acc: 0.960 | test-loss: 0.582 | test-acc: 0.800
Epoch 9000: training-loss: 0.129 | training-acc: 0.960 | test-loss: 0.620 | test-acc: 0.800
Epoch 10000: training-loss: 0.122 | training-acc: 0.960 | test-loss: 0.662 | test-acc: 0.800
Epoch 0: training-loss: 0.698 | training-acc: 0.490 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.610 | training-acc: 0.740 | test-loss: 0.598 | test-acc: 0.760
Epoch 2000: training-loss: 0.436 | training-acc: 0.850 | test-loss: 0.451 | test-acc: 0.800
Epoch 3000: training-loss: 0.326 | training-acc: 0.860 | test-loss: 0.401 | test-acc: 0.830
Epoch 4000: training-loss: 0.288 | training-acc: 0.870 | test-loss: 0.409 | test-acc: 0.830
Epoch 5000: training-loss: 0.276 | training-acc: 0.880 | test-loss: 0.429 | test-acc: 0.830
Epoch 6000: training-loss: 0.271 | training-acc: 0.880 | test-loss: 0.448 | test-acc: 0.830
Epoch 7000: training-loss: 0.268 | training-acc: 0.880 | test-loss: 0.462 | test-acc: 0.830
Epoch 8000: training-loss: 0.266 | training-acc: 0.870 | test-loss: 0.474 | test-acc: 0.830
Epoch 9000: training-loss: 0.265 | training-acc: 0.870 | test-loss: 0.481 | test-acc: 0.830
Epoch 10000: training-loss: 0.264 | training-acc: 0.870 | test-loss: 0.487 | test-acc: 0.830
Epoch 0: training-loss: 0.700 | training-acc: 0.513 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.552 | training-acc: 0.793 | test-loss: 0.557 | test-acc: 0.770
Epoch 2000: training-loss: 0.424 | training-acc: 0.873 | test-loss: 0.448 | test-acc: 0.780
Epoch 3000: training-loss: 0.360 | training-acc: 0.887 | test-loss: 0.397 | test-acc: 0.800
Epoch 4000: training-loss: 0.327 | training-acc: 0.907 | test-loss: 0.389 | test-acc: 0.820
Epoch 5000: training-loss: 0.312 | training-acc: 0.900 | test-loss: 0.393 | test-acc: 0.810
Epoch 6000: training-loss: 0.305 | training-acc: 0.887 | test-loss: 0.401 | test-acc: 0.840
Epoch 7000: training-loss: 0.295 | training-acc: 0.880 | test-loss: 0.409 | test-acc: 0.820
Epoch 8000: training-loss: 0.289 | training-acc: 0.880 | test-loss: 0.412 | test-acc: 0.830
Epoch 9000: training-loss: 0.287 | training-acc: 0.880 | test-loss: 0.413 | test-acc: 0.830
Epoch 10000: training-loss: 0.285 | training-acc: 0.887 | test-loss: 0.413 | test-acc: 0.830
Epoch 0: training-loss: 0.696 | training-acc: 0.500 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.522 | training-acc: 0.845 | test-loss: 0.533 | test-acc: 0.820
Epoch 2000: training-loss: 0.373 | training-acc: 0.850 | test-loss: 0.400 | test-acc: 0.840
Epoch 3000: training-loss: 0.329 | training-acc: 0.860 | test-loss: 0.374 | test-acc: 0.840
Epoch 4000: training-loss: 0.317 | training-acc: 0.860 | test-loss: 0.375 | test-acc: 0.840
Epoch 5000: training-loss: 0.312 | training-acc: 0.855 | test-loss: 0.380 | test-acc: 0.830
Epoch 6000: training-loss: 0.309 | training-acc: 0.855 | test-loss: 0.385 | test-acc: 0.830
Epoch 7000: training-loss: 0.308 | training-acc: 0.860 | test-loss: 0.390 | test-acc: 0.830
Epoch 8000: training-loss: 0.307 | training-acc: 0.860 | test-loss: 0.394 | test-acc: 0.830
Epoch 9000: training-loss: 0.307 | training-acc: 0.860 | test-loss: 0.397 | test-acc: 0.820
Epoch 10000: training-loss: 0.306 | training-acc: 0.860 | test-loss: 0.399 | test-acc: 0.820
Epoch 0: training-loss: 0.699 | training-acc: 0.484 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.568 | training-acc: 0.752 | test-loss: 0.563 | test-acc: 0.800
Epoch 2000: training-loss: 0.416 | training-acc: 0.856 | test-loss: 0.441 | test-acc: 0.840
Epoch 3000: training-loss: 0.365 | training-acc: 0.868 | test-loss: 0.403 | test-acc: 0.860
Epoch 4000: training-loss: 0.346 | training-acc: 0.868 | test-loss: 0.395 | test-acc: 0.850
Epoch 5000: training-loss: 0.339 | training-acc: 0.872 | test-loss: 0.394 | test-acc: 0.850
Epoch 6000: training-loss: 0.336 | training-acc: 0.876 | test-loss: 0.395 | test-acc: 0.840
Epoch 7000: training-loss: 0.335 | training-acc: 0.880 | test-loss: 0.397 | test-acc: 0.850
Epoch 8000: training-loss: 0.334 | training-acc: 0.884 | test-loss: 0.398 | test-acc: 0.850
Epoch 9000: training-loss: 0.334 | training-acc: 0.884 | test-loss: 0.398 | test-acc: 0.840
Epoch 10000: training-loss: 0.333 | training-acc: 0.884 | test-loss: 0.398 | test-acc: 0.840
Epoch 0: training-loss: 0.700 | training-acc: 0.500 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.533 | training-acc: 0.813 | test-loss: 0.530 | test-acc: 0.790
Epoch 2000: training-loss: 0.406 | training-acc: 0.850 | test-loss: 0.406 | test-acc: 0.810
Epoch 3000: training-loss: 0.371 | training-acc: 0.850 | test-loss: 0.377 | test-acc: 0.800
Epoch 4000: training-loss: 0.362 | training-acc: 0.843 | test-loss: 0.373 | test-acc: 0.800
Epoch 5000: training-loss: 0.359 | training-acc: 0.850 | test-loss: 0.373 | test-acc: 0.810
Epoch 6000: training-loss: 0.358 | training-acc: 0.853 | test-loss: 0.374 | test-acc: 0.810
Epoch 7000: training-loss: 0.358 | training-acc: 0.853 | test-loss: 0.374 | test-acc: 0.810
Epoch 8000: training-loss: 0.357 | training-acc: 0.853 | test-loss: 0.374 | test-acc: 0.810
Epoch 9000: training-loss: 0.357 | training-acc: 0.853 | test-loss: 0.374 | test-acc: 0.810
Epoch 10000: training-loss: 0.357 | training-acc: 0.853 | test-loss: 0.374 | test-acc: 0.810
Epoch 0: training-loss: 0.698 | training-acc: 0.494 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.543 | training-acc: 0.809 | test-loss: 0.538 | test-acc: 0.800
Epoch 2000: training-loss: 0.411 | training-acc: 0.811 | test-loss: 0.407 | test-acc: 0.820
Epoch 3000: training-loss: 0.373 | training-acc: 0.829 | test-loss: 0.375 | test-acc: 0.830
Epoch 4000: training-loss: 0.359 | training-acc: 0.829 | test-loss: 0.367 | test-acc: 0.830
Epoch 5000: training-loss: 0.355 | training-acc: 0.834 | test-loss: 0.363 | test-acc: 0.830
Epoch 6000: training-loss: 0.354 | training-acc: 0.834 | test-loss: 0.362 | test-acc: 0.830
Epoch 7000: training-loss: 0.353 | training-acc: 0.834 | test-loss: 0.361 | test-acc: 0.830
Epoch 8000: training-loss: 0.352 | training-acc: 0.834 | test-loss: 0.360 | test-acc: 0.830
Epoch 9000: training-loss: 0.351 | training-acc: 0.834 | test-loss: 0.362 | test-acc: 0.830
Epoch 10000: training-loss: 0.351 | training-acc: 0.834 | test-loss: 0.361 | test-acc: 0.830
Epoch 0: training-loss: 0.700 | training-acc: 0.502 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.563 | training-acc: 0.805 | test-loss: 0.557 | test-acc: 0.820
Epoch 2000: training-loss: 0.432 | training-acc: 0.812 | test-loss: 0.423 | test-acc: 0.850
Epoch 3000: training-loss: 0.389 | training-acc: 0.818 | test-loss: 0.390 | test-acc: 0.860
Epoch 4000: training-loss: 0.379 | training-acc: 0.825 | test-loss: 0.386 | test-acc: 0.860
Epoch 5000: training-loss: 0.375 | training-acc: 0.830 | test-loss: 0.388 | test-acc: 0.860
Epoch 6000: training-loss: 0.374 | training-acc: 0.830 | test-loss: 0.388 | test-acc: 0.860
Epoch 7000: training-loss: 0.374 | training-acc: 0.828 | test-loss: 0.389 | test-acc: 0.860
Epoch 8000: training-loss: 0.373 | training-acc: 0.830 | test-loss: 0.389 | test-acc: 0.860
Epoch 9000: training-loss: 0.373 | training-acc: 0.830 | test-loss: 0.389 | test-acc: 0.860
Epoch 10000: training-loss: 0.373 | training-acc: 0.828 | test-loss: 0.389 | test-acc: 0.870
Epoch 0: training-loss: 0.702 | training-acc: 0.491 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.551 | training-acc: 0.804 | test-loss: 0.546 | test-acc: 0.820
Epoch 2000: training-loss: 0.432 | training-acc: 0.813 | test-loss: 0.432 | test-acc: 0.830
Epoch 3000: training-loss: 0.393 | training-acc: 0.822 | test-loss: 0.406 | test-acc: 0.830
Epoch 4000: training-loss: 0.378 | training-acc: 0.820 | test-loss: 0.389 | test-acc: 0.830
Epoch 5000: training-loss: 0.375 | training-acc: 0.820 | test-loss: 0.390 | test-acc: 0.830
Epoch 6000: training-loss: 0.374 | training-acc: 0.822 | test-loss: 0.391 | test-acc: 0.830
Epoch 7000: training-loss: 0.374 | training-acc: 0.820 | test-loss: 0.392 | test-acc: 0.830
Epoch 8000: training-loss: 0.374 | training-acc: 0.820 | test-loss: 0.393 | test-acc: 0.830
Epoch 9000: training-loss: 0.374 | training-acc: 0.820 | test-loss: 0.393 | test-acc: 0.830
Epoch 10000: training-loss: 0.374 | training-acc: 0.820 | test-loss: 0.393 | test-acc: 0.830
Epoch 0: training-loss: 0.701 | training-acc: 0.490 | test-loss: 0.697 | test-acc: 0.500
Epoch 1000: training-loss: 0.562 | training-acc: 0.790 | test-loss: 0.553 | test-acc: 0.810
Epoch 2000: training-loss: 0.411 | training-acc: 0.848 | test-loss: 0.413 | test-acc: 0.840
Epoch 3000: training-loss: 0.353 | training-acc: 0.856 | test-loss: 0.372 | test-acc: 0.840
Epoch 4000: training-loss: 0.337 | training-acc: 0.856 | test-loss: 0.364 | test-acc: 0.840
Epoch 5000: training-loss: 0.331 | training-acc: 0.858 | test-loss: 0.364 | test-acc: 0.830
Epoch 6000: training-loss: 0.329 | training-acc: 0.858 | test-loss: 0.366 | test-acc: 0.830
Epoch 7000: training-loss: 0.328 | training-acc: 0.858 | test-loss: 0.367 | test-acc: 0.830
Epoch 8000: training-loss: 0.327 | training-acc: 0.860 | test-loss: 0.369 | test-acc: 0.830
Epoch 9000: training-loss: 0.327 | training-acc: 0.860 | test-loss: 0.369 | test-acc: 0.830
Epoch 10000: training-loss: 0.326 | training-acc: 0.858 | test-loss: 0.369 | test-acc: 0.830
